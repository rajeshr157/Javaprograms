Since a decade, “Big Data” has become the ubiquitous term to describe all the data that is generated from their smartphones, web browsing history, social media and purchasing behavior, together with any other information that organizations hold about them.

Why is big data different to any other type of data? In one sense, there isn’t a difference; it’s all just zeros and ones at the end of the day. However, the term “Big Data” tends to be applied to large collections of different types of data which are often volatile and changeable, and where one would struggle to analyse it using traditional computer hardware and software.

In particular, big data includes:

Text Data. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output..

Images.  This covers photos and video, as well as medical imaging. One application of machine learning is to use features identified in scans and x-rays to predict the likelihood that someone has a specific disease.

Social network data. This is information about people’s connections and who they know. Network data includes the number and type of connections that people have, as well as data about connected individuals. If all your friends are sci-fi geeks, that’s probably a good indication that you might be one too.

Geospatial. Information about peoples’ location and movements, provided by smart phones and other mobile devices.

Machine Data. Everyday devices from televisions to coffee makers are being designed to share information between themselves and over the internet. These days your heating, kettle, washing machine, and so on can be all be controlled via your smart phone. The “Internet of Things” (IoT) concept is still developing, but will eventually provide lots of data that can be used to infer people’s behavior using machine learning 

In today’s world of big data, data is being updated much more frequently, often in real time. In addition, a lot more of it is “free form” unstructured data such as speech, e-mails, tweets, blogs, and so on. Another factor is that much of this data is often generated independently of the organization that wants to use it. This is problematic, because if data is captured or generated by an organization itself, then they can control how that data is formatted, and put checks and controls in place to ensure that the data is accurate and complete. However, if data is being generated from external sources then there are no guarantees that the data is correct.
In particular, modern big data computer systems are good at:

1. Storing massive amounts of data. Traditional databases are limited in the amount of data that they can hold at reasonable cost. New ways of storing data as allowed an almost limitless expansion in cheap storage capacity.

2. Data cleaning and formatting. Diverse and messy data needs to be transformed into a standard format before it can be used for machine learning, management reporting, or other data related tasks.
Processing data quickly. Big data is not just about there being more data. It needs to be processed and analysed quickly to be of greatest use.
The issue with traditional computer systems wasn’t that there was any theoretically barrier to them undertaking the processing required to utilize big data, but in practice they were too slow, too cumbersome and too expensive to do so.

3. New data storage and processing paradigms such as Hadoop/MapReduce have enabled tasks which would have taken weeks or months to process to be undertaken in just a few hours, and at a fraction of the cost of more traditional data processing approaches. The way that Hadoop does this is to allow data and data processing to be spread across networks of cheap desktop PCs. In theory, tens of thousands of PCs can be connected together to deliver massive computational capabilities that are comparable to the largest supercomputers in existence.

Data (whether “Big” or “Small”) has no intrinsic value in itself. A big mistake that an organization can make is to think that if they invest in a mass storage system, such as Hadoop, and collect every scrap of data they can about people, then that’s going to add value. The data has to be worked into something useful if it’s going to be of benefit. Machine learning is the key tool that does that – applying algorithms to all that data and producing predictive models that can tell you something about people’s behavior, based on what has happened before in the past.

A good way to think about the relationship between big data and machine learning is that the data is the raw material that feeds the machine learning process. The tangible benefit to a business is derived from the predictive model(s) that comes out at the end of the process, not the data used to construct it.